[
  {
    "objectID": "communication/2020-12-22-modelito-sabanero/index.html",
    "href": "communication/2020-12-22-modelito-sabanero/index.html",
    "title": "Mi Modelito Sabanero",
    "section": "",
    "text": "Ayer fue #CienciaPasión2020. Como #CienciaVisión, un escaparate del humor, la música y el amor por la ciencia a todos los niveles, a los que @InmaPToro y un servidor tuvieron a bien invitarnos. El objetivo: cantar una noticia del año. La nuestra: GPT-3\n\n\n\nPero antes, queremos agradecer de CORAZÓN a la peñita maravillosa que organizan todas estas MARAVILLAS de iniciativas y que nos llaman pa mamarrachear al compás: @bynzelman@maitecicleta@hayquehacerla@PutoMikel@Nebesu_@ConchiLillo@manolux4444@sassyscience_@garirius@Victagua (fijo que me dejo alguien)\nY ahora paso a comentar algo que me han preguntado en varias ocasiones: ¿De qué co**es va la letra de la canción? Este «modelito» sabanero no es otro que GPT-3, un modelo de deep learning para el lenguaje natural que desde su publicación ha revolucionado el campo. ¡Vamos por frases!\n\n«Con mi burrito sabanero voy entrenando mi modelo,\nred neuronal del lenguaje natural».\n\nSon redes neuronales (una de las bases de la inteligencia artificial) especializadas en comprender y entender el lenguaje de los seres humanos -&gt; el «lenguaje natural».\n\n«Con tensorflow voy programando, el modelo voy compilando,\ncapa tras capa vi’apilando, el modelo voy compilando\nGPT, GPT, se llama GPT-3″\n\nTensorflow es la librería más usada para programar redes neuronales. Aunque yo soy más de PyTorch. Pero a día de hoy es el estándar de programación y en producción. Casi todos los grandes avances se pueden encontrar preparados para Tensorflow.\nEn estos modelos de deep learning, las neuronas se estructuran por «capas», grupos de neuronas que se conectan con otras capas. En nuestro caso, palabras. Si las neuronas fueran alumnos que aprenden, las capas serían filas, como comentaba en Famelab.\nY GPT-3 es el sucesor de GPT-2 y 1, modelos de redes neuronales creados para modelar el lenguaje natural, basados en una arquitectura llamada «transformer». Y que fueron un bombazo por artículos como este, completamente escritos por una Int. Artificial.\n\n«Libro tras libro descargando, la wikipedia escrapeando, todas las palabras en vectores a la entrada y a la salida»\n\nGPT-3 se entrenó utilizando millones de libros de dominio público, artículos científicos y la wikipedia al completo. Referencias: https://arxiv.org/abs/2005.14165\n\n«Atención, atención, mecanismo de atención»\n\n\n\n\nMecanismo de atención en imágenes\n\n\nLa gran innovación de los transformers es el «mecanismo de atención», una herramienta que se construye en las redes neuronales, que permite a la red centrarse en unas entradas más que en otras, les presta más «atención».\nEl mecanismo de auto-atención (self-attention) se presentó en el paper «Attention is all you need», en el NeurIPS 2017, paper que, por cierto, también aparece en el vídeo: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n\n«Tuki tuki tuki tuki,_\ntuki tuki tuki ta\najusta bien el modelo que lo vamos a entrenar»\n\nTremendo cumbión. Pues eso, a preparar un modelo de red «tó polluo», que si no, no funciona.\n\n«Tuki tuki tuki tuki,\ntuki tuki tuki ta,\na ver si pasa el test de Turing, que lo vamos a petar»\n\nEl test de Turing es un test diseñado por Alan Turing, que permitiría decir cuando una máquina es inteligente. Esto sucedía en un entorno experimental en el que humanos interactuaban vía texto en conversación con otros humanos, o podría ser con máquinas. Si un ser humano era incapaz de discernir si estaba hablando con una máquina o con otro ser humano, se dice que ese algoritmo ha pasado el Test de Turing.\n\n«Con mi burrito sabanero vi’entrenando mi transformer\nEn arXiv, en arXiv, puedes encontrarme allí\nen github, en github puedes encontrarme allá»\n\nhttp://arXiv.org es una web donde muchos científicos publican sus avances antes de enviarlos a revistas. Son conocidos como pre-prints, y aunque todavía no han pasado el proceso de revisión, es una forma muy rápida (y gratuita) de acceder al conocimiento que se está produciendo.\nY Github, pues eso.. @github Un servidor de Git, un sistema de control de versiones donde los desarrolladores de software centralizar el control de los cambios que hacen a su código. Allí están centralizados en «repositorios», lugares desde donde después se puede distribuir el código y contribuir a una ciencia más abierta.\n\n«Muy pronto los veremos integraos en una aplicación\ndentro del Office generando textos con gran precisión\ny en robots, en robots, robots de conversación»\n\nLa aplicación directa: integrar los modelos para generar, aumentar productividad e interactúar con humanos. Ya hay modelos similares que se están implementando en aplicaciones como Photoshop para ayudar en la productividad de imagen. Y para el lenguaje natural, herramientas como DeepL translate ya tienen una gran difusión para traducir textos. Es sólo cuestión de tiempo que se integren en los paquetes de software más usados.\n\n«Y con lo caro que se ha puesto el precio del kilovatio\ny lo que gasta en entrenarlo, Endesa se está forrando.\nGPU, GPU, la factura de la luz»\n\nEl consumo energético: ésta es la cara B de todo el boom del Deep Learning. El consumo de las GPU, o las unidades de procesamiento gráfico que se utilizan en redes neuronales es mucho más elevado que el de los procesadores normales. Se dice que entrenar GPT-3 consumió tanto como conducir un coche de la tierra a la luna, y volver.\n\n«Perrea con Ada Lovelace, lo baila Alan Turing,\nlo está bailando Hinton perreando a LeCun.\nPerreamos tan a ritmo nos cargamos la ley de Moore»\n\nAda Lovelace (1815-1852) se considera la primera programadora de la historia. Fue hija del poeta inglés Lord Byron, y de la también matemática y activista social Anna Isabella Byron. Fue educada, entre otras, por la matemática Mary Somerville. Tuvo una buena relación con Charles Babbage, a quien ayudó a diseñar el funcionamiento teórico de la «máquina diferencial», describiendo en sus «Notas» con un lenguaje muy técnico cómo funcionaría esta máquina, distinguiendo con claridad los conceptos de datos y procesamiento. Pero para más datos, este hilo es genial:\nDe Alan Turing, poco se puede decir que no se haya dicho ya. Padre de la Inteligencia Artificial, descifró Enigma (la máquina de los Nazis) y fue condenado al ostracismo por su propio gobierno al descubrir su homosexualidad. Acabó suicidándose.\nPor último, Hinton y LeCun son considerados «padres» de la explosión del deep learning.\nCon esto, y un bizcocho… ¡A PERREAR, PERREAR LA INTELIGENCIA ARTIFICIAL!\nGracias a los que habéis llegado hasta aquí, y a los que habéis hecho posible esta canción, este hilo y esta alegría que llevo dentro desde #cienciapasion2020 !!!\n\n\n\nCitationBibTeX citation:@online{martinez-murcia2020,\n  author = {Martinez-Murcia, F.J.},\n  title = {Mi {Modelito} {Sabanero}},\n  date = {2020-12-20},\n  url = {https://pakitochus.github.io/fjmartinezmurcia.es/COMMUNICATION/2020-12-22-modelito-sabanero/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMartinez-Murcia, F.J. 2020. “Mi Modelito Sabanero.”\nDecember 20, 2020. https://pakitochus.github.io/fjmartinezmurcia.es/COMMUNICATION/2020-12-22-modelito-sabanero/."
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "Communication",
    "section": "",
    "text": "Mi Modelito Sabanero\n\n\n\ncienciavision\n\n\nmusica\n\n\ncomunicacion\n\n\nia\n\n\n\nUna oda a GPT-3\n\n\n\nF.J. Martinez-Murcia\n\n\nDec 20, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Currently I research in Information Technology, inside the interdisciplinary field of Computer Aided Diagnosis, Image-based diagnosis and Statistical and Digital Signal Processing. A provisional title for all this research is Computed Aided Diagnosis for the detection of Neurodegenerative Disorders using Medical Imaging.\nDuring this period I have belonged to the Signal Processing and Biomedical Applications (SiPBA) research group, and I have written or contributed to several publications.\n\n\nResearch projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrainSimulator\n\n\n\npython\n\n\nproject\n\n\n\nFunctional brain image synthesis using the KDE or MVN distribution.\n\n\n\nF.J. Martinez-Murcia\n\n\nNov 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoctoral Thesis\n\n\n\npython\n\n\nmatlab\n\n\nthesis\n\n\n\nFStatistical Neuroimage Modeling, Processing and Synthesis based on Texture and Component Analysis: Tackling the Small Sample Size Problem..\n\n\n\nF.J. Martinez-Murcia\n\n\nJun 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSignificance Weighted Principal Component Analysis (SWPCA)\n\n\n\npython\n\n\nproject\n\n\n\nSignificance Weighted Principal Component Analysis (SWPCA) is a technique (1) developed to parse out the influence of a categorical variable that introduces variability in a…\n\n\n\nF.J. Martinez-Murcia\n\n\nOct 24, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpherical Brain Mapping\n\n\n\npython\n\n\nmatlab\n\n\nproject\n\n\n\nThe Spherical Brain Mapping (SBM) is a feature extraction and visualization framework intended to map the internal structures and features of the brain onto a 2D image that…\n\n\n\nF.J. Martinez-Murcia\n\n\nSep 15, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/2020-04-14-CyS-UMA/index.html",
    "href": "teaching/2020-04-14-CyS-UMA/index.html",
    "title": "Circuitos y Sistemas - Canal de Youtube",
    "section": "",
    "text": "Playlist\nEn 2020 impartía la segunda parte de la asignatura de Circuitos y Sistemas, en la E.T.S. de Ingeniería de Telecomunicación de la Universidad de Málaga. Dado el impacto de la pandemia y confinamiento mediante, no se podía realizar de modo presencial. Así que había dos opciones: conectarse a meet/zoom o generar contenido más elaborado en forma de vídeos docentes.\nAl final (pobre ignorante) opté por esta segunda opción, para dejar que los alumnos y demás interesados pudieran acceder al contenido bajo demanda, y creé el canal de youtube UMA-CYS-GISE, acrónoimo de «Circuitos y Sistemas» – Grado en Ingeniería de Sistemas Electrónicos.\nEl resultado es una serie de vídeos cubriendo los temas que me habrían tocado en su momento, comenzando por el análisis de la respuesta en frecuencia de un circuito electrónico, y pasando por diferentes temas como los filtros de diferentes tipologías y órdenes, los diagramas de bode, implementaciones electrónicas y el análisis de cuadripolos."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Teaching I do\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial Deep Learning con Pytorch\n\n\n\ndocencia\n\n\npython\n\n\ndeep learning\n\n\ncomputer vision\n\n\n\nTutorial de pytorch para la asignatura Tratamiento Digital de Voz e Imagen, del grado en Sonido de Imagen de la Universidad de Málaga..\n\n\n\nF.J. Martinez-Murcia\n\n\nMay 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCircuitos y Sistemas - Canal de Youtube\n\n\n\ndocencia\n\n\nmatlab\n\n\npython\n\n\ncircuitos\n\n\nsistemas\n\n\n\nMaterial docente audiovisual para el seguimiento virtual y bajo demanda de la asignatura Circuitos y Sistemas de la Universidad de Malaga.\n\n\n\nF.J. Martinez-Murcia\n\n\nApr 14, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/2020-04-14-CyS-UMA/index.html#tema-3",
    "href": "teaching/2020-04-14-CyS-UMA/index.html#tema-3",
    "title": "Circuitos y Sistemas - Canal de Youtube",
    "section": "Tema 3",
    "text": "Tema 3\n\nTema 3 - Cap 1 - Respuesta en Frecuencia\nTema 3 - Cap 2 - ¿Qué es un filtro?\nTema 3 - Cap 3 - Filtro de paso baja de primer orden\nTema 3 - Cap 4 - Filtro de paso alta de primer orden\nTema 3 - Cap 5 - Filtros de Paso Baja 2º Orden\nTema 3 - Cap 6 - Filtros de paso alto de 2º orden\nTema 3 - Cap 7 - Filtros de paso banda de 2º orden\nTema 3 - Cap 8 - Filtros de banda eliminada de 2º orden\nTema 3 - Cap 9 - Diagramas de Bode (introducción)\nTema 3 - Cap 10 - Diagrama de Bode con puntos singulares reales\nTema 3 - Cap 11 - Diagramas de Bode con puntos singulares complejos conjugados\nTema 3 - Cap 12 - Diseño de filtros a partir del diagrama de Bode\nTema 3 - Cap 13 - Respuesta en frecuencia del amplificador operacional"
  },
  {
    "objectID": "teaching/2020-04-14-CyS-UMA/index.html#tema-4",
    "href": "teaching/2020-04-14-CyS-UMA/index.html#tema-4",
    "title": "Circuitos y Sistemas - Canal de Youtube",
    "section": "Tema 4",
    "text": "Tema 4\n\nTema 4 - Cap 1 - Bipuertos\nTema 4 - Cap 2 - Parámetros G y H\nTema 4 - Cap 3 - Conexión en serie\nTema 4 - Cap 4 - Análisis de circuitos con bipuertos"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "F.J. Martinez-Murcia",
    "section": "",
    "text": "Telecommunications Engineer and PhD in Information and Communication Technologies from the University of Granada. Currently, Ramon y Cajal researcher at the University of Granada, specialising in signal processing and analysis and medical brain imaging. Finalist in Famelab 2018 and winner of 3-Minute Thesis Granada 2017. Musician and hopeless geek."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "F.J. Martinez-Murcia",
    "section": "Education",
    "text": "Education\n\nTelecommunications Engineer (2010) | University of Granada\nMsC in Computer and Network Engineering (2011) | University of Granada\nPhD in Information and Communications Technology (2017) | University of Granada"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "F.J. Martinez-Murcia",
    "section": "Experience",
    "text": "Experience\n\nWebsite manager, front-end development (2010-2011) | Language4you education S.L.\nPredoctoral researcher (2014-2017) | University of Granada\nPostdoctoral researcher (2018-2019) | DaSCI – Andalusian Institute on Data Science and Computational Intelligence\nJuan de la Cierva – formación fellow (2019-2020) | University of Malaga\nJose Castillejo fellow (2019) | Ludwig-Maximilien Universität München\nJuan de la Cierva – incorporación fellow (2020-2022) | University of Granada\nRamon y Cajal fellow (2023 - ) | University of Granada\nCAS Research Group Fellow (2023 - ) | Ludwig-Maximilien Universität München"
  },
  {
    "objectID": "teaching/2021-10-01-DeepLearning/index.html",
    "href": "teaching/2021-10-01-DeepLearning/index.html",
    "title": "Tutorial Deep Learning con Pytorch",
    "section": "",
    "text": "Formato libro  Repositorio\nEn 2020 tuve la suerte de poder realizar una práctica de introducción al Deep Learning utilizando pytorch para que lo usaran los alumnos del grado en Sonido e Imagen de la Universidad de Málaga.\nPosteriormente lo edité en formato libro (bastante fácil siendo todos notebooks de jupyter) y lo colgué en github.\n\nOrganización del tutorial\nEste tutorial consta de tres partes, cada una de ellas es un notebook de jupyter, o sea, un documento como el que estás visualizando en este momento. Las partes son las siguientes:\n\nParte 0: Introducción al Deep Learning. Es este notebook, preparatorio de la práctica mediante realización guiada en el que se tratarán:\n\nUnas pinceladas a las matemáticas y la historia de las redes neuronales\nInformación sobre qué es y cómo instalar el software necesario para esta práctica: la distribución de python 3 Anaconda, la librería de computación tensorial PyTorch y la plataforma de notebooks jupyter.\nUna introducción a la computación matricial en python con pytorch, que en muchas cosas se parece a matlab.\n\nParte 1: El Perceptrón Multicapa. En esta parte se introduce el perceptrón multicapa (Multi-Layer Perceptron o MLP), la red neuronal más básica que podemos construir. Se verán detalladamente las pautas a seguir para crear, entrenar y evaluar una red neuronal utilizando la librería pytorch. Será una realización guiada, sin problemas a resolver.\nParte 2: Redes Neuronales Convolucionales. Esta parte será la entregable de la práctica de redes. En ella veremos las redes neuronales convolucionales, que son la herramienta más potente que existe a día de hoy para el procesado de imagen. Se utilizan en coches autónomos, en la búsqueda de imágenes de google, diagnóstico de enfermedades, interpretación del lenguaje, y muchas más aplicaciones. Veremos las particularidades de estas redes y cómo se implementan cada una. La segunda parte de este notebook será el problema a resolver: crear y entrenar una red convolucional para la detección de dígitos escritos a mano.\n\n\n\n\n\nCitationBibTeX citation:@online{martinez-murcia2020,\n  author = {Martinez-Murcia, F.J.},\n  title = {Tutorial {Deep} {Learning} Con {Pytorch}},\n  date = {2020-05-11},\n  url = {https://pakitochus.github.io/fjmartinezmurcia.es/teaching/2021-10-01-DeepLearning/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMartinez-Murcia, F.J. 2020. “Tutorial Deep Learning Con\nPytorch.” May 11, 2020. https://pakitochus.github.io/fjmartinezmurcia.es/teaching/2021-10-01-DeepLearning/."
  }
]